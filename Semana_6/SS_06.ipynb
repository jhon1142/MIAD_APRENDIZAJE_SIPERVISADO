{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"figs/ans_banner_1920x200.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 6. Sesión Sincrónica.\n",
    "\n",
    "\n",
    "El  *cuaderno* tiene dos objetivos:\n",
    " 1. Introducir a los estudiantes a los contenidos de la semana 6.\n",
    " 2. Introducir a los estudiantes al análisis de datos espaciales que se estudiarán en la semana 7.\n",
    "\n",
    "**NO** es necesario editar el archivo o hacer una entrega. Los ejemplos contienen celdas con código ejecutable (`en gris`), que podrá modificar libremente. Esta puede ser una buena forma de aprender nuevas funcionalidades del *cuaderno*, o experimentar variaciones en los códigos de ejemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué son los sistemas de recomendaciones?\n",
    "\n",
    "\n",
    "Las preferencias de los individuos suelen seguir patrones que los sistemas de recomendación pueden aprovechar, por ejemplo,\n",
    "\n",
    "- Si te interesó:  <div style=\"max-width:200px\">\n",
    "<img src = \"figs/iron_man.jpg\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "- También te puede interesar: <div style=\"max-width:200px\">\n",
    "<img src = \"figs/thor.jpg\" />\n",
    "</div> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Los sistemas de recomendación entonces encuentran patrones que son utilizados para predecir qué otros productos podrían gustarnos y generar sugerencias, de forma tal que  los usuarios encuentren contenido atractivo en un gran corpus. \n",
    "\n",
    "- Estos sistemas son muy exitosos, por ejemplo según un estudio del 2013 de [McKinsey](https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers), el 35% de los artículos comprados en Amazon surgen de estos sistemas de recomendación, y por lo tanto vale la pena estudiarlos cuidadosamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrado Colaborativo Basado en Usuarios.\n",
    "\n",
    "El filtrado colaborativo aprovecha el poder de la colaboración para generar recomendaciones. \n",
    "\n",
    "\n",
    "Para entender un poco mejor cuál es el problema al que nos enfrentamos, supongamos que tenemos una matriz con 5 usuarios y 5 productos, en este caso restaurantes. \n",
    "\n",
    "El valor de la celda denota el rating que le dió cada usuario al ítem. Este valor lo denotamos como $r_{ij}$ que será entonces el rating que le dio el usuario $i$ al restaurante $j$. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo: Recomendando Restaurantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos librerias\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos y visualizamos  los datos\n",
    "ratings = pd.read_csv('data/Ratings.csv', encoding='latin-1')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_matrix = ratings.pivot_table(values='rating', index='user_id', columns='restaurant_id')\n",
    "r_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregando Información Espacial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos y visualizamos  los datos\n",
    "usuarios = pd.read_csv('data/Usuarios.csv')\n",
    "usuarios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restaurantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos y visualizamos  los datos\n",
    "restaurants = pd.read_csv('data/Restaurants.csv')\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación a geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos geopandas que es la librería a utilizar\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = gpd.GeoDataFrame(restaurants, geometry=gpd.points_from_xy(restaurants.longitud, restaurants.latitud),crs=4326)\n",
    "restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuarios = gpd.GeoDataFrame(usuarios, geometry=gpd.points_from_xy(usuarios.longitud, usuarios.latitud),crs=4326)\n",
    "usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización interactiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "map = folium.Map(location = [4.65283,-74.054339], tiles = \"OpenStreetMap\", zoom_start = 16)\n",
    "# Otras opciones de tiles\n",
    "#Stamen Terrain, Toner, and Watercolor\n",
    "\n",
    "\n",
    "for i in range(0,len(restaurants)):\n",
    "   folium.Marker(\n",
    "      location=[restaurants.iloc[i]['latitud'], restaurants.iloc[i]['longitud']],\n",
    "       popup=restaurants.iloc[i]['nombre'],\n",
    "   ).add_to(map)\n",
    "\n",
    "for i in range(0,len(usuarios)):\n",
    "   folium.Marker(\n",
    "      location=[usuarios.iloc[i]['latitud'], usuarios.iloc[i]['longitud']],\n",
    "       popup=usuarios.iloc[i]['nombre'], icon=folium.Icon(color='red')\n",
    "   ).add_to(map)\n",
    "    \n",
    "\n",
    "#Display el mapa\n",
    "map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Midiendo Distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuarios.geometry.apply(lambda g: restaurants.distance(g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proyecciones: La tierra no es plana!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:700px\">\n",
    "<img src = \"figs/proj.png\" />\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://gifs.com/gif/mercator-projection-y4xP7j', width=700, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculamos la distancia\n",
    "\n",
    "[MAGNA-SIRGAS](https://epsg.io/3116): Marco Geocéntrico Nacional de Referencia,densificación del Sistema de Referencia Geocéntrico para las Américas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuarios= usuarios.to_crs(3116)\n",
    "restaurants= restaurants.to_crs(3116)\n",
    "usuarios.geometry.apply(lambda g: restaurants.distance(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"max-width:500px\">\n",
    "<img src = \"figs/dist1.png\" />\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrado Colaborativo Basado en Contenido. Análisis de textos\n",
    "\n",
    "\n",
    "- Los sistemas basados en contenido, a diferencia de los filtros colaborativos, no requieren datos relacionados a otros individuos u actividades pasadas. \n",
    "\n",
    "\n",
    "- Por el contrario, estos brindan recomendaciones basadas en el perfil del usuario y los metadatos que se tiene sobre elementos particulares.\n",
    "\n",
    "\n",
    "- Parte importante viene de la descripción de los items y esta descripción esta por lo general expresada en texto.\n",
    "\n",
    "\n",
    "- Visitaremos entonces brevemente como aprovechar esta información para mejorar las recomendaciones.\n",
    "\n",
    "\n",
    "- Para ello tenemos que encontrar el significado de las expresiones\n",
    "\n",
    "    - El principio de composicionalidad, que tiene su origen en la filosofía del lenguaje, dice que el significado de una expresión es una función de los significados de sus subexpresiones. \n",
    "    - El significado de una oración es una función de los significados de sus frases, el significado de una frase es una función de los significados de sus palabras, y el significado de una palabra es una función de los significados de sus lexemas. \n",
    "\n",
    "\n",
    "- Habiendo determinado la estructura de un fragmento de texto, encontrar su significado puede verse como un proceso de abajo hacia arriba, desde los lexemas en la parte inferior pasando por los morfemas, las oraciones, los temas, y más allá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos librerias\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cargamos y visualizamos  los datos\n",
    "ratings = pd.read_csv('data/Ratings.csv', encoding='latin-1')\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=ratings.dropna()\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Necesitamos procesar el texto para que sea útil y poder capturar significado de las palabras a través de los morfemas.\n",
    "\n",
    "- Vamos a realizar unos pasos previos\n",
    "\n",
    "    - Remover elementos como stopwords, tildes, signos de puntuación, números, y cualquier otro símbolo que pueden llegar a entorpecer el análisis. En este paso las expresiones regulares son especialmente útiles. \n",
    "    \n",
    "  - Separar el texto en palabras o *tokens*\n",
    "  \n",
    "  - Encontrar los morfemas o lexemas via *lematización* o *stemming*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comencemos con la limpieza de los comentarios\n",
    "\n",
    "- Voy a mostrarles como hacerlo con la librería de procesamiento y análisis de texto, ConTexto, desarrollada por el DNP de Colombia.\n",
    "\n",
    "\n",
    "- Ver https://ucd-dnp.github.io/ConTexto/versiones/master/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ratings['comentarios'][12]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librerías a utilizar ()\n",
    "from contexto.limpieza import *\n",
    "\n",
    "from contexto.correccion import Corrector, corregir_texto\n",
    "\n",
    "\n",
    "texto_corregido = corregir_texto(ratings['comentarios'][12])\n",
    "print(texto_corregido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para quitar acentos (diéresis, tildes y virgulillas)\n",
    "texto_corregido=remover_acentos(texto_corregido)\n",
    "texto_corregido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargamos las stopwords extra\n",
    "extra_stopwords = pd.read_csv('data/extra_stopwords.csv', sep=',',header=None)\n",
    "extra_stopwords.columns = ['stopwords']\n",
    "extra_stopwords=set(extra_stopwords['stopwords'].to_list())\n",
    "\n",
    "# Agregamos a nuestro modelo de SpaCy\n",
    "nlp.Defaults.stop_words |= extra_stopwords\n",
    "\n",
    "#print(nlp.Defaults.stop_words) # Mostrar la nueva lista de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corremos limpieza de texto\n",
    "texto_corregido=limpieza_texto(texto_corregido,lista_palabras=extra_stopwords)\n",
    "texto_corregido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lematización\n",
    " \n",
    "- Operación que consiste en transformar las palabras de su forma flexionada (plural, femenino, conjugaciones, etc.) a su lema correspondiente, el cual es el representante de todas las formas flexionadas de una misma palabra. \n",
    " \n",
    " \n",
    " - Por ejemplo, las palabras niños, niña y niñito tienen todas el mismo lema: niño. \n",
    " \n",
    " \n",
    " - Realizar lematización sobre textos puede simplificarlos, al unificar palabras que comparten el mismo lema, y evitando así tener un vocabulario más grande de lo necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La primera vez que se utilice una combinación particular de lenguaje + tamaño, la librería descargará el modelo correspondiente en el computador del usuario. \n",
    "#Para usarlo, se debe reiniciar la sesión y correr la función de nuevo.\n",
    "from contexto.lematizacion import LematizadorSpacy\n",
    "from contexto.lematizacion import lematizar_texto\n",
    "\n",
    "\n",
    "texto_corregido = lematizar_texto(texto_corregido)\n",
    "\n",
    "texto_corregido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar lemas desde un diccionario\n",
    "\n",
    "segundo_dict = {\n",
    "    \"llego\" : \"llegar\",\n",
    "    \"deliciós\": \"delicioso\",\n",
    "    \"comido\": \"comida\",\n",
    "    \"malo\":\"mal\"\n",
    "    \n",
    "}\n",
    "texto_lematizado = lematizar_texto(texto_corregido, dict_lemmas=segundo_dict)\n",
    "print(texto_lematizado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming \n",
    "\n",
    "- El stemming es un método para reducir todas las formas flexionadas de palabras a su «raíz» o «tallo» (stem, en inglés), cuando estas comparten una misma raíz.\n",
    "\n",
    "\n",
    "- Por ejemplo, las palabras niños, niña y niñez tienen todas la misma raíz: «niñ». A diferencia de la lematización, en donde cada lema es una palabra que existe en el vocabulario del lenguaje correspondiente, las palabras raíz que se obtienen al aplicar stemming no necesariamente existen por sí solas como palabra.\n",
    "\n",
    "\n",
    "- Aplicar stemming a textos puede simplificarlos, al unificar palabras que comparten la misma raíz, y evitando así tener un vocabulario más grande de lo necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contexto.stemming import Stemmer, stem_texto\n",
    "\n",
    "# Determinar automáticamente el lenguaje del texto\n",
    "texto_stem = stem_texto(texto_lematizado, 'auto')\n",
    "print(texto_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos definir todas juntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['comentarios']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que limpia el texto y separa en tokens\n",
    "def text_cleaning(txt):\n",
    "\n",
    "    out = corregir_texto(txt)\n",
    "    out= remover_acentos(out)\n",
    "    out= limpieza_texto(out,lista_palabras=extra_stopwords)\n",
    "    out=lematizar_texto(out)\n",
    "    out=lematizar_texto(out, dict_lemmas=segundo_dict)\n",
    "#    out=stem_texto(out, 'auto')\n",
    "    return out\n",
    "\n",
    "\n",
    "clean = ratings['comentarios'].apply(text_cleaning)\n",
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridad entre las comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contexto.comparacion import Similitud, Distancia, DiferenciaStrings\n",
    "from contexto.vectorizacion import *\n",
    "\n",
    "# Bolsa de palabras\n",
    "v_bow = VectorizadorFrecuencias()\n",
    "v_bow.ajustar(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(v_bow.vocabulario().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitud de coseno\n",
    "s_bow = Similitud(v_bow)\n",
    "coseno_bow = s_bow.coseno(clean)\n",
    "coseno_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelado de tópicos\n",
    "\n",
    "- Contar palabras puede ser una forma bastante buena de cuantificar de qué se trata un documento, pero no es perfecta.\n",
    "\n",
    "\n",
    "- Supongamos que estamos tratando de categorizar documentos en diferentes géneros. \n",
    "\n",
    "\n",
    "- Si un documento contiene la palabra \"láser\" o \"robot\" u otras palabras similares, esa es una buena señal de que es una palabra de ingeniería  o una historia de ciencia ficción. \n",
    "\n",
    "\n",
    "- Supongamos además que muchas de estas palabras relacionadas con la tecnología simplemente ocurren una vez, digamos, \"fotón\" aparece solo en un artículo de ingeniería y \"Warp\" aparece solo en un historia de ciencia ficción. \n",
    "\n",
    "\n",
    "- Esa no sería una muy buena evidencia de que los documentos que mencionan \"fotón\" son ingeniería y los documentos que mencionan \"Warp\" son ciencia ficción. \n",
    "\n",
    "\n",
    "- Sería más razónable que un modelo aprenda que todas estas son palabras relacionadas con la tecnología, y que las palabras relacionadas con la tecnología  son indicativas tanto de ingeniería como de ciencia ficción.\n",
    "\n",
    "\n",
    "- Es decir, estas palabras pertenecen al tema de la tecnología, y tanto la ingeniería como la ciencia ficción usan estas palabras.\n",
    "\n",
    "\n",
    "- ¿Cómo podríamos aprender esos temas automáticamente? La respuesta más utilizada es LDA\n",
    "\n",
    "\n",
    "- La idea detrás LDA, es responder ¿qué pasa si usamos vectores donde cada componente del vector es un tema diferente? \n",
    "\n",
    "\n",
    "\n",
    "- En estos casos estamos buscando encontrar de forma no supervizada cual es el factor (tópico o tema) subyacente del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Ejemplo paso a paso\n",
    "\n",
    "En este ejemplo vamos a asumir que hay 3 documentos (D), que contienen 3 temas (K) y 7 (V) palabras\n",
    "\n",
    "- Paso 1: Definir el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = [\"gato\", \"perro\", \"vacuna\", \"hospital\", \"dinero\", \"banco\", \"impuestos\"]\n",
    "V = len(vocabulario)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Por qué definir el vocabulario?*\n",
    "El vocabulario es el conjunto de palabras que pueden aparecer en nuestros documentos. Definirlo es fundamental porque todas las distribuciones de tópicos y palabras se basan en este conjunto. En LDA, cada palabra generada pertenece al vocabulario, y las probabilidades de cada tópico se asignan sobre estas palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 2: Definir las distribuciones de palabras por tópico (φ)\n",
    "\n",
    "Estas salen de una Dirichlet, mas sobre esto abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_1 = [0.4, 0.4, 0.05, 0.05, 0.03, 0.03, 0.04]  # Animales\n",
    "phi_2 = [0.05, 0.05, 0.4, 0.4, 0.03, 0.03, 0.04]  # Salud\n",
    "phi_3 = [0.05, 0.05, 0.05, 0.05, 0.35, 0.35, 0.10]  # Economía\n",
    "phi = [phi_1, phi_2, phi_3]\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 3: Definir mezcla de tópicos del documento (θ)\n",
    "\n",
    "Ahora si veamos como funciona la Dirichlet. Esta distribución es una de las pocas distribuciones que modela directamente vectores de probabilidades. Es decir, vectores cuyas componentes:\n",
    "\n",
    "- son no negativas,\n",
    "\n",
    "- y suman exactamente 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "# Paso 3: Definir mezcla de tópicos del documento (θ)\n",
    "np.random.seed(123)\n",
    "D = 3  # número de documentos\n",
    "alpha = [1, 1, 20]  # K=3\n",
    "\n",
    "# Generar muestras de la Dirichlet\n",
    "samples = dirichlet.rvs(alpha, size=D)\n",
    "df = pd.DataFrame(samples, columns=[\"Topico1\", \"Topico2\", \"Topico3\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_d = df.iloc[0].values  # de arriba\n",
    "print(theta_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Graficar puntos de masa \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(df[\"Topico1\"], df[\"Topico2\"], df[\"Topico3\"], color=\"blue\", alpha=0.5, s=50)\n",
    "ax.set_xlabel(\"Tópico 1\")\n",
    "ax.set_ylabel(\"Tópico 2\")\n",
    "ax.set_zlabel(\"Tópico 3\")\n",
    "plt.title(\"Muestras de una Dirichlet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 4: Número de palabras en el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 5: Generar los tópicos z_1, ..., z_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.choice([1, 2, 3], size=N, replace=True, p=theta_d)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 6: Generar palabras según los tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "for n in range(N):\n",
    "  topic_idx = z[n] - 1  # z es 1-indexado, Python usa 0-indexado\n",
    "  word = np.random.choice(vocabulario, size=1, p=phi[topic_idx])[0]\n",
    "  w.append(word)\n",
    "\n",
    "# Resultado: palabras generadas\n",
    "pd.DataFrame({\"Posición\": range(1, N+1), \"Palabra\": w, \"Tópico\": z})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso 7: Bag of Words (conteo por palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(w)\n",
    "bow_ordered = {word: bow.get(word, 0) for word in vocabulario}\n",
    "print(bow_ordered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.barplot(x=list(bow_ordered.keys()), y=list(bow_ordered.values()))\n",
    "plt.title(\"Frecuencia de palabras (Bag of Words)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.xlabel(\"Palabra\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingenieria reversa\n",
    "\n",
    "Hasta ahora, simulamos documentos **generando** palabras. Pero en la realidad en la pratica\n",
    "\n",
    "* Un corpus de documentos reales → textos observados.\n",
    "* Una representación en BoW o DTM.\n",
    "\n",
    "\n",
    "Lo que buscamos es inferir los **parámetros** del modelo:\n",
    "\n",
    "  * Las mezclas de tópicos por documento $\\theta_d$,\n",
    "  * Las distribuciones de palabras por tópico $\\phi_k$,\n",
    "  * Las asignaciones de tópicos $z_{d,n}$ para cada palabra.\n",
    "\n",
    "Es la ingenieria reversa de MLE: queremos encontrar los parámetros que hacen más **verosímil** (más probable) haber observado nuestros documentos.\n",
    "\n",
    "\n",
    "$$\n",
    "\\max_{\\theta, \\phi} \\ p(\\text{documentos} \\mid \\theta, \\phi)\n",
    "$$\n",
    "\n",
    "Pero en LDA no observamos los $z_{d,n}$, ni $\\theta$, ni $\\phi$. \n",
    "\n",
    "**Solo observamos las palabras.**\n",
    "\n",
    "El problema a resolver es la siguiente verosimilitud:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w} \\mid \\alpha, \\beta) = \\int \\sum_{\\mathbf{z}} p(\\mathbf{w}, \\mathbf{z}, \\theta, \\phi \\mid \\alpha, \\beta) \\ d\\theta \\ d\\phi\n",
    "$$\n",
    "\n",
    "Este término **no se puede calcular exactamente**: la suma sobre todas las posibles asignaciones de tópicos $\\mathbf{z}$ es **exponencial**.\n",
    "\n",
    "¿Por qué es \"exponencial\"?\n",
    "\n",
    "Porque si: hay $N$ palabras en total en el corpus,  y cada palabra puede tener **uno de $K$ tópicos**, entonces hay: $K^N$ posibles combinaciones de asignaciones de tópicos. Ejemplo: supongamos\n",
    "\n",
    "* 2 documentos, cada uno con 10 palabras → $N = 20$,\n",
    "* 3 posibles tópicos → $K = 3$\n",
    "\n",
    "Entonces hay:\n",
    "\n",
    "$$\n",
    "3^{20} = 3,486,784,401\n",
    "$$\n",
    "\n",
    "combinaciones posibles de $\\mathbf{z}$. \n",
    "\n",
    "Entonces\n",
    "\n",
    "* No podemos **sumar sobre todas** esas posibilidades explícitamente.\n",
    "* Por eso necesitamos métodos que **aproximen esa suma**:\n",
    "\n",
    "\n",
    "En la practica hay varios métodos de **inferencia aproximada**, que estiman los parámetros.\n",
    "\n",
    "Los dos más comunes:\n",
    "\n",
    "1. **Gibbs Sampling** (una forma de MCMC)\n",
    "2. **Variational Inference** (una versión determinista, más rápida)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresamos a nuestro ejemplo de los comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la función \n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "#helper function\n",
    "def splitter(txt):\n",
    "    out = txt.split(\" \")\n",
    "    return out\n",
    "\n",
    "clean = clean.apply(splitter)\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la representación de diccionario del documento\n",
    "dictionary = Dictionary(clean)\n",
    "dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in clean]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "Estimacion = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=4,\n",
    "    chunksize=1000,\n",
    "    passes=20,\n",
    "    iterations=400,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    random_state=123,\n",
    "    eval_every=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(Estimacion.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LdaModel.get_document_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[Estimacion.get_document_topics(item) for item in corpus]\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones sobre LDA\n",
    "\n",
    "\n",
    "- Cuando se analiza el modelado de tópicos, es importante reforzar continuamente el hecho de que los grupos de palabras que representan los tópicos no están relacionados conceptualmente; están relacionados solo por proximidad. \n",
    "\n",
    "\n",
    "- La proximidad frecuente de ciertas palabras en los documentos es suficiente para definir tópicos debido a que estamos asumiendo que todas las palabras en el mismo documento están relacionadas.\n",
    "\n",
    "\n",
    "- Sin embargo, esta suposición puede no ser cierta o las palabras pueden ser demasiado genéricas para formar tópicos coherentes. \n",
    "\n",
    "\n",
    "- La interpretación de tópicos abstractos implica equilibrar las características innatas de los datos de texto con las agrupaciones de palabras generadas. \n",
    "\n",
    "\n",
    "- Es importante tener en cuenta también que la naturaleza ruidosa de los datos de texto puede hacer que los modelos de temas asignen palabras no relacionadas con uno de los tópicos a ese tema en particular. \n",
    "\n",
    "\n",
    "- Recuerden \"GIGO\" (garbage in, garbage out) si entra basura, sale basura. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Información de Sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "\n",
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "150.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
